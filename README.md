# Sample Cassandra REST-API


### Development Installation

Install and start Cassandra either in Docker as in later steps, or have it running locally.
By default a local single node on localhost is assumed. You can also connect to a cluster.
If you want to connect to an external node or cluster you can add it in a see [_configuraton_](#configuration) file or set
a environemnt variable called `CASSANDRA_HOSTS`

Start cassandra:
```
cassandra -f
```
Install app depemndencies with [pipenv](https://docs.pipenv.org/)
```
pipenv install --dev
```

And then launch the API
```
pipenv run server
```
The description of the endpoints can be seen at the root of the server for instance:
(http://localhost:5000/) where the individual endpoints can be opened and tried out. Both Cassandra and the server needs to be running.


## <a name="configuration"></a>Configuration
The default configuration settings are located in `example_config.cfg`.
The settings can be overriden by pointing the environment variable `FLASK_APP_CONFIG_FILE` to a similar config file.


# Testing
Requires you run it locally with pipenv
## Unit testing and ci-testing
For Unit testing, run:
```
pipenv run test
```
The tests come in two parts: unit testing with pytest, and api testing using `tavern-ci`
which tests the REST-endpoints. Both are run by pytest using the previous command.
Unit testing uses its own "testing" keyspace which is reset every run, the api testing uses the development keyspace that the flask app
is also uses locally.

## Verification

Get sample data to the project root folder:
```
wget https://cdn.example.com/sample_data.jsonl.bz2 -O - | bunzip2 > sample-data.jsonl
```
Download the [sample data]( https://cdn.example.com/sample_data.jsonl.bz2) and place it as `sample_data.jsonl`  in the root folder of the project.
Import a couple of thousand of them by running. This is not mandarory for unit testing but for [verifying](http://localhost:5000/) that the apis work and for the travis-ci part of the tests it is needed.
```
pipenv shell
python tests/prepopulate.py sample-data.jsonl 10000
```

Go to [the api docs](http://localhost:5000/) and use the "try out" section for the different endpoints

# Docker

### Run Cassandra in Docker.
Launch a Cassandra node if you dont have one, here we save the id to a variable to use in the next step ...
```
CASSANDRA_HOST_ID=`docker run -d -t cassandra:3`
```
We get the ip of the cassandra node and add it to a varialbe that we can use in the next step.

```
CASSANDRA_HOSTS=`docker inspect -f '{{ .NetworkSettings.IPAddress }}' $CASSANDRA_HOST_ID`
```
check that it worked:
```
echo $CASSANDRA_HOSTS
```
>> 172.17.0.4


The project will use that node if it finds an environment variable called `CASSANDRA_HOSTS`. It may be a comma separated list of nostnames or ip-addresses

### Running the app in Docker

Build an image of current project:
```
docker build -t sample-api .
```
Run it and pass the cassandra ip as an environment variable:
```
docker run -p 8000:8080 --env CASSANDRA_HOSTS=$CASSANDRA_HOSTS -t sample-api
```


# Technologies used

The services is built on top Flask and Flask-resplus, a lightweight Python framework for building REST-apis and webservices.
The connection to Cassandra nodes is done via the python-driver for Cassandra and via Flask-CQLAlchemy.
Flask-CQLAlchemy provides an nice integration to the Cassandra models provided by python-driver.
The actual database models are described using Model classes (in app/models/) that desctibe the Cassandra tables.
a sync command generates the Cassandra tables.

## Cassandra tables
The tables, which were generated by the Cassandra python-driver can be seen in a generalized form here:
```
CREATE KEYSPACE player_sessions WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '3'}
AND durable_writes = true;

CREATE TABLE player_sessions.completed_sessions_by_player_id (
    player_id uuid,
    session_id uuid,
    end_ts timestamp,
    country text,
    start_ts timestamp,
    PRIMARY KEY (player_id, session_id, end_ts)
) WITH CLUSTERING ORDER BY (session_id ASC, end_ts DESC)


CREATE TABLE player_sessions.session_starts_by_country (
    country text,
    daybucket text,
    start_ts timestamp,
    player_id uuid,
    session_id uuid,
    PRIMARY KEY ((country, daybucket), start_ts)
) WITH CLUSTERING ORDER BY (start_ts DESC)


CREATE TABLE player_sessions.session_events_by_player_id (
    player_id uuid,
    session_id uuid,
    ts timestamp,
    event text,
    country text,
    PRIMARY KEY (player_id, session_id, ts, event)
) WITH CLUSTERING ORDER BY (session_id ASC, ts DESC, event ASC)
```
The complete cql dump can be seen in app/models/schema.cql
I decided to create a combined paritition key for the table session_starts_by_country since they need to be
queried based on country and hours back. Therefore i decided to enable part of the dates to function as about
bucket to allow for an even partition accross nodes. The daybucket consists of the date of the event.

# API description
The task was to design and implement a player session service which consumes events and provides metrics about players sessions.
Each user will generate two events, one start event when the session starts and one end event when session is finished.
When both events have been received the session is considered complete. Service is expected to handle massive amount of sessions.

## Requirements
- Use Python and Cassandra
- All endpoints are REST APIs
- API for receiving event batches (1-10 events / batch)
- API for fetching session starts for the last X (X is defined by the user) hours for each country API for fetching last 20 complete sessions for a given player
- Data older than 1 year should be discarded

These requirements are fulfilled. The data which is inserted has a TTL of one year.

# Additional notes
There is a branch for containerizing this project using Docker but it is not finished.
> This project was very interesting and i am looking forward to feedback. - Christoffer Bj√∂rkskog